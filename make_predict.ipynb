{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50f7b155",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-20T23:32:28.147891Z",
     "start_time": "2023-04-20T23:32:26.312389Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm, trange\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from sklearn.calibration import calibration_curve\n",
    "from scipy.special import expit\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig, AdamW\n",
    "import matplotlib.patches as mpatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36a54e46",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-20T23:32:28.163391Z",
     "start_time": "2023-04-20T23:32:28.148892Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers.utils import logging\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a3fe1d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-20T23:32:28.178891Z",
     "start_time": "2023-04-20T23:32:28.164892Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_ids_mask(sentences, tokenizer, max_length):\n",
    "    tokenized = [\n",
    "        tokenizer.encode(s, add_special_tokens=True, max_length=100, truncation=True, return_tensors='pt')[0]\n",
    "        for s in sentences\n",
    "    ]\n",
    "    \n",
    "    ids = pad_sequence(tokenized, batch_first=True)\n",
    "    amasks = (ids > 0).float()\n",
    "\n",
    "    return ids, amasks\n",
    "\n",
    "def load_cm_sentences(split=\"train\", has_labels=False):\n",
    "    df = pd.read_csv(f\"{split}.csv\")\n",
    "    \n",
    "    sentences = df['input'].tolist()\n",
    "    \n",
    "    if not has_labels:\n",
    "        return sentences\n",
    "    \n",
    "    labels = df['label'].tolist()\n",
    "    \n",
    "    return sentences, labels\n",
    "\n",
    "def load_process_data(model_name, max_length, split=\"train\", has_labels=False):\n",
    "    data = load_cm_sentences(split=split, has_labels=has_labels)\n",
    "    \n",
    "    if has_labels:\n",
    "        sentences, labels = data\n",
    "        labels = torch.tensor(labels)\n",
    "    else:\n",
    "        sentences = data\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    ids, amasks = get_ids_mask(sentences, tokenizer, max_length)\n",
    "    \n",
    "    if has_labels:\n",
    "        data = TensorDataset(ids, amasks, labels)\n",
    "    else:\n",
    "        data = TensorDataset(ids, amasks)\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eea975b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-20T23:32:28.194391Z",
     "start_time": "2023-04-20T23:32:28.179892Z"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_loaders(model_name, batch_size, max_length, add_val=False):\n",
    "    loaders = dict()\n",
    "    \n",
    "    train_data = load_process_data(model_name, max_length, 'train', has_labels=True)\n",
    "    test_data = load_process_data(model_name, max_length, 'test')\n",
    "\n",
    "    if add_val:\n",
    "        train_data, val_data = torch.utils.data.random_split(train_data, [0.85, 0.15])\n",
    "        loaders['val'] = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    loaders['train'] = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    loaders['test'] = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "    return loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8697cb5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-20T23:32:28.209892Z",
     "start_time": "2023-04-20T23:32:28.195392Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_model(model_name, learning_rate, weight_decay, cache_dir=None):\n",
    "    if cache_dir is not None:\n",
    "        config = AutoConfig.from_pretrained(model_name, num_labels=1, cache_dir=cache_dir)\n",
    "    else:\n",
    "        config = AutoConfig.from_pretrained(model_name, num_labels=1)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, config=config)\n",
    "\n",
    "    model.cuda()\n",
    "\n",
    "    no_decay = ['bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in model.named_parameters()\n",
    "                    if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay': weight_decay},\n",
    "        {'params': [p for n, p in model.named_parameters()\n",
    "                    if any(nd in n for nd in no_decay)],\n",
    "         'weight_decay': 0.0}\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate, eps=1e-8, no_deprecation_warning=True)\n",
    "\n",
    "    return model, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31fc6f08",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-20T23:32:28.225392Z",
     "start_time": "2023-04-20T23:32:28.210892Z"
    }
   },
   "outputs": [],
   "source": [
    "def dump_pred(y_pred, threshold, save_path):\n",
    "    if y_pred.min() < 0 or y_pred.max() > 1:\n",
    "        raise ValueError('Wrong pred values!!!')\n",
    "        \n",
    "    if y_pred.shape[0] != 2771:\n",
    "        raise ValueError('Wrong pred size!!!')\n",
    "        \n",
    "    neg_scale = 1 / threshold\n",
    "    pos_scale = 1 / (1 - threshold)\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'class': (y_pred > threshold).astype('int'),\n",
    "        'uncertainty': np.where(y_pred < threshold, y_pred * neg_scale, (1 - y_pred) * pos_scale)\n",
    "    })\n",
    "    \n",
    "    if df['uncertainty'].min() < 0 or df['uncertainty'].max() > 1:\n",
    "        raise ValueError('Wrong pred result!!!')\n",
    "    \n",
    "    df.to_csv(save_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4925bc9d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-20T23:32:28.240892Z",
     "start_time": "2023-04-20T23:32:28.226392Z"
    }
   },
   "outputs": [],
   "source": [
    "class Predictor:\n",
    "    def __init__(self, model_name, config, n_splits=3):\n",
    "        self.model_name = model_name\n",
    "        self.loaders = prepare_loaders(\n",
    "            model_name=model_name,\n",
    "            batch_size=config['batch_size'],\n",
    "            max_length=config['max_length'],\n",
    "        )\n",
    "        self.config = config\n",
    "        self.criterion = nn.BCEWithLogitsLoss()\n",
    "        \n",
    "    def _train(self, n_epochs, loader):\n",
    "        model, optimizer = load_model(\n",
    "            model_name=self.model_name,\n",
    "            learning_rate=self.config['learning_rate'],\n",
    "            weight_decay=self.config['weight_decay']\n",
    "        )\n",
    "        model.train()\n",
    "        gradient_acc_steps = self.config['gradient_acc_steps']\n",
    "        \n",
    "        for _ in trange(n_epochs, desc='Epoch'):\n",
    "            for step, batch in enumerate(tqdm(loader), 1):\n",
    "                batch = tuple(t.cuda() for t in batch)\n",
    "                ids, amasks, labels = batch\n",
    "                \n",
    "                output = model(ids, attention_mask=amasks)[0].squeeze(-1)\n",
    "                loss = self.criterion(output, labels.float()) / gradient_acc_steps\n",
    "                \n",
    "                loss.backward()\n",
    "                \n",
    "                if (step % gradient_acc_steps == 0) or (step == len(train_dataloader)):\n",
    "                    # Update weights\n",
    "                    optimizer.step()\n",
    "\n",
    "                    # Zero gradient buffers\n",
    "                    optimizer.zero_grad()\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def _eval(self, model, loader):\n",
    "        model.eval()\n",
    "        y_pred = []\n",
    "        y_true = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for step, batch in enumerate(loader, 1):\n",
    "                batch = tuple(t.cuda() for t in batch)\n",
    "                ids, amasks, labels = batch\n",
    "                \n",
    "                output = model(ids, attention_mask=amasks)[0].squeeze(-1)\n",
    "                \n",
    "                y_pred.append(output.cpu().numpy())\n",
    "                y_true.append(labels.cpu().numpy())\n",
    "        \n",
    "        y_pred = expit(np.hstack(y_pred))\n",
    "        y_true = np.hstack(y_true)\n",
    "        \n",
    "        return y_pred, y_true\n",
    "    \n",
    "    def _predict(self, model, loader):\n",
    "        model.eval()\n",
    "        y_pred = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for step, batch in enumerate(loader, 1):\n",
    "                batch = tuple(t.cuda() for t in batch)\n",
    "                ids, amasks = batch\n",
    "                \n",
    "                output = model(ids, attention_mask=amasks)[0].squeeze(-1)\n",
    "                \n",
    "                y_pred.append(output.cpu().numpy())\n",
    "        \n",
    "        y_pred = expit(np.hstack(y_pred))\n",
    "        \n",
    "        return y_pred\n",
    "    \n",
    "    def train_predict(self, n_epochs):\n",
    "        model = self._train(n_epochs, self.loaders['train'])\n",
    "    \n",
    "        y_pred = self._predict(model, self.loaders['test'])\n",
    "        \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87752235",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-20T23:50:44.277031Z",
     "start_time": "2023-04-20T23:32:28.242392Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Zelgades\\anaconda3\\envs\\NLP\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:447: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  \"The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6295d3e8bdb9433f8eba68dc7d188926",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "921c1c7b2de34f1e8262093afa629cc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/870 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e7b1cebe36b47b1b2c3dae7e3102ab8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/870 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e431d26aa2954d0db9f5a84c5d6d74c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/870 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5039fcfd286446669da0c9c3d752937e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/870 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = 'microsoft/deberta-v3-large'\n",
    "config = {\n",
    "    'batch_size': 16,\n",
    "    'max_length': 512,\n",
    "    'learning_rate': 1e-5,\n",
    "    'weight_decay': 0.01,\n",
    "    'gradient_acc_steps': 1\n",
    "}\n",
    "\n",
    "scorer = Predictor(model_name, config)\n",
    "\n",
    "y_pred = scorer.train_predict(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef63f427",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-20T23:50:44.292531Z",
     "start_time": "2023-04-20T23:50:44.278531Z"
    }
   },
   "outputs": [],
   "source": [
    "dump_pred(y_pred, 0.5, 'submissions/submission_0.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd34a2d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-20T23:50:44.308031Z",
     "start_time": "2023-04-20T23:50:44.295031Z"
    }
   },
   "outputs": [],
   "source": [
    "class CalibratedPredictor(Predictor):\n",
    "    def __init__(self, model_name, config, n_splits=3):\n",
    "        self.model_name = model_name\n",
    "        self.loaders = prepare_loaders(\n",
    "            model_name=model_name,\n",
    "            batch_size=config['batch_size'],\n",
    "            max_length=config['max_length'],\n",
    "            add_val=True\n",
    "        )\n",
    "        self.config = config\n",
    "        self.criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    def train_predict(self, n_epochs):\n",
    "        model = self._train(n_epochs, self.loaders['train'])\n",
    "        \n",
    "        val_pred, val_true = self._eval(model, self.loaders['val'])\n",
    "        \n",
    "        calibrator = IsotonicRegression(y_min=0, y_max=1, out_of_bounds='clip')\n",
    "        calibrator.fit(val_pred, val_true)\n",
    "    \n",
    "        test_pred = self._predict(model, self.loaders['test'])\n",
    "        test_pred = calibrator.predict(test_pred)\n",
    "        \n",
    "        return test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08e850c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T00:06:55.737484Z",
     "start_time": "2023-04-20T23:50:44.309532Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "940b0594fc92438db9e169a47532aee2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06215c6d099d4677b24c09850da2c260",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/739 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3118879fcb84146815eeb82aa099021",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/739 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb463aeae3184795836f2f24eac27e2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/739 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd2b1e6580b749b394cddb79cf1b8d26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/739 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = 'microsoft/deberta-v3-large'\n",
    "config = {\n",
    "    'batch_size': 16,\n",
    "    'max_length': 512,\n",
    "    'learning_rate': 1e-5,\n",
    "    'weight_decay': 0.01,\n",
    "    'gradient_acc_steps': 1\n",
    "}\n",
    "\n",
    "scorer = CalibratedPredictor(model_name, config)\n",
    "\n",
    "y_pred = scorer.train_predict(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7c7a163",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T00:06:55.752984Z",
     "start_time": "2023-04-21T00:06:55.738984Z"
    }
   },
   "outputs": [],
   "source": [
    "dump_pred(y_pred, 0.5, 'submissions/submission_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff41fa7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
